---
title: "Untitled"
output: html_document
---
  
File name is:
`elasticsearch-hadoop-7.6.2.jar`

Here is the maven link:

https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop/7.6.2

Triplet:

```
org.elasticsearch
elasticsearch-hadoop
7.6.2
````

## SBT repository
```
// https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop
libraryDependencies += "org.elasticsearch" % "elasticsearch-hadoop" % "7.6.2"
```


## Example code

```
import org.elasticsearch.spark._

val conf = ...
val sc = new SparkContext(conf)

val numbers = Map("one" -> 1, "two" -> 2, "three" -> 3)
val airports = Map("OTP" -> "Otopeni", "SFO" -> "San Fran")

sc.makeRDD(Seq(numbers, airports)).saveToEs("spark/docs")
```
https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop/7.6.2
https://github.com/elastic/elasticsearch-hadoop/blob/master/README.md


You can always use configuration files or --conf argument to spark-shell 
to set required parameters which will be used be the default context. 
In case of Kryo you should take a look at:



```{scala}
val conf = new SparkConf().setAppName("basic_a").setMaster("local")

conf.set("es.index.auto.create", "true")
conf.set("es.nodes")
conf.set("es.port","9200")

val sc = new SparkContext(conf)
```

esconf["es.nodes"] = "localhost"
esconf["es.port"] = "9200"


conf = SparkConf().setAppName("updateSchools")
sc = SparkContext(conf=conf)

# from the spark shell
import org.apache.spark.SparkContext
val sc = SparkContext.getOrCreate()


